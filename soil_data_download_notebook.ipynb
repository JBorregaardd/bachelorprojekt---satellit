{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sep 25 23:51:15 2023\n",
    "\n",
    "@author: mabso\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages read all below\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentinelhub import SHConfig\n",
    "from sentinelhub.geometry import BBox, Geometry\n",
    "from shapely.geometry import Polygon, mapping, MultiPolygon\n",
    "import shutil\n",
    "import glob\n",
    "import datetime\n",
    "import os\n",
    "import tifffile as tiff\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sentinelhub import CRS, BBox, DataCollection, MimeType, WcsRequest, WmsRequest\n",
    "import rasterio\n",
    "import re\n",
    "\n",
    "from sentinelhub import (\n",
    "    CRS,\n",
    "    BBox,\n",
    "    DataCollection,\n",
    "    DownloadRequest,\n",
    "    MimeType,\n",
    "    MosaickingOrder,\n",
    "    SentinelHubDownloadClient,\n",
    "    SentinelHubRequest,\n",
    "    bbox_to_dimensions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up using your sentinelhub account\n",
    "\n",
    "# Set your API credentials\n",
    "CLIENT_ID = \"ca2fd726-2237-4a0e-9ee8-8dbcb0caf3ef\"\n",
    "CLIENT_SECRET = \"0vDMaibWNiaWQ9VXaGySAXEszqvOkgP6\"\n",
    "\n",
    "# Configure Sentinel Hub\n",
    "config = SHConfig()\n",
    "config.sh_client_id = CLIENT_ID\n",
    "config.sh_client_secret = CLIENT_SECRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in your data\n",
    "\n",
    "# read in shape files (id number and geometrics shape - lon and lat coordinate pairs)\n",
    "file_path=\"C:/Users/Jacob pc/vscode_projects/bachelorprojekt---satellit/LUCAS_2018_Copernicus/LUCAS_2018_Copernicus_polygons.shp\"\n",
    "df_shape=gpd.read_file(file_path)\n",
    "\n",
    "# read in the soil file (id number and lots of soil properties per id number)\n",
    "df_soil=pd.read_csv(\"LUCAS_2018_Copernicus/LUCAS-SOIL-2018.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set your evalscripts\n",
    "\n",
    "# this is a float ndvi script - you can also compute these manually from raw data\n",
    "evalscript_all_nvdi = \"\"\"\n",
    "   //VERSION=3\n",
    "    function setup() {\n",
    "      return{\n",
    "        input: [{\n",
    "          bands: [\"B04\", \"B08\"]\n",
    "        }],\n",
    "        output: {\n",
    "          id: \"default\",\n",
    "          bands: 1,\n",
    "          sampleType: SampleType.FLOAT32\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    function evaluatePixel(sample) {\n",
    "      let ndvi = (sample.B08 - sample.B04) / (sample.B08 + sample.B04)\n",
    "      return [ ndvi ]\n",
    "    }\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling month script\n",
    "def samp_month_fun(mon_num):\n",
    "    \n",
    "    if mon_num=='01':\n",
    "        out=\"2018-01-01\", \"2018-01-31\"\n",
    "    \n",
    "    elif mon_num=='02':\n",
    "        out=\"2018-02-01\", \"2018-02-28\"\n",
    "        \n",
    "    elif mon_num=='03':\n",
    "         out=\"2018-03-01\", \"2018-03-31\"\n",
    "    \n",
    "    elif mon_num=='04':\n",
    "        out=\"2018-04-01\", \"2018-04-30\"\n",
    "    \n",
    "    elif mon_num=='05':\n",
    "        out=\"2018-05-01\", \"2018-05-31\"\n",
    "    \n",
    "    elif mon_num=='06':\n",
    "        out=\"2018-06-01\", \"2018-06-30\"\n",
    "    \n",
    "    elif mon_num=='07':\n",
    "        out=\"2018-07-01\", \"2018-07-31\"\n",
    "    \n",
    "    elif mon_num=='08':\n",
    "        out=\"2018-08-01\", \"2018-08-31\"\n",
    "    \n",
    "    elif mon_num=='09':\n",
    "        out=\"2018-09-01\", \"2018-09-30\"\n",
    "    \n",
    "    elif mon_num=='10':\n",
    "        out=\"2018-10-01\", \"2018-10-31\"\n",
    "    \n",
    "    elif mon_num=='11':\n",
    "        out=\"2018-11-01\", \"2018-11-30\"\n",
    "        \n",
    "    elif mon_num=='12':\n",
    "        out=\"2018-12-01\", \"2018-12-31\"\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        26461768\n",
      "1        26501768\n",
      "2        26521762\n",
      "3        26521776\n",
      "4        26521978\n",
      "           ...   \n",
      "63282    64981670\n",
      "63283    64981672\n",
      "63284    65001668\n",
      "63285    65001672\n",
      "63286    65021668\n",
      "Name: POINT_ID, Length: 63287, dtype: int32\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_complex_request' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 100\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m SentinelHubRequest(\n\u001b[0;32m     80\u001b[0m             evalscript\u001b[38;5;241m=\u001b[39mevalscript_all_nvdi, \u001b[38;5;66;03m# set your evalscript to what you want\u001b[39;00m\n\u001b[0;32m     81\u001b[0m             data_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msat_download\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# change to where you want your data output\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m             config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Create a list of requests\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     list_of_requests \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     99\u001b[0m             get_NDWI_request(slot) \u001b[38;5;28;01mfor\u001b[39;00m slot \u001b[38;5;129;01min\u001b[39;00m slots\n\u001b[1;32m--> 100\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m    101\u001b[0m             get_complex_request(slot) \u001b[38;5;28;01mfor\u001b[39;00m slot \u001b[38;5;129;01min\u001b[39;00m slots  \u001b[38;5;66;03m# remove one of these if only wanting to download one thing at at time - usally a good idea to download multiple to speed up\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         ]\n\u001b[0;32m    103\u001b[0m     list_of_requests \u001b[38;5;241m=\u001b[39m [request\u001b[38;5;241m.\u001b[39mdownload_list[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m request \u001b[38;5;129;01min\u001b[39;00m list_of_requests]\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# Download data with multiple threads\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 101\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m SentinelHubRequest(\n\u001b[0;32m     80\u001b[0m             evalscript\u001b[38;5;241m=\u001b[39mevalscript_all_nvdi, \u001b[38;5;66;03m# set your evalscript to what you want\u001b[39;00m\n\u001b[0;32m     81\u001b[0m             data_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msat_download\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# change to where you want your data output\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m             config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Create a list of requests\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     list_of_requests \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     99\u001b[0m             get_NDWI_request(slot) \u001b[38;5;28;01mfor\u001b[39;00m slot \u001b[38;5;129;01min\u001b[39;00m slots\n\u001b[0;32m    100\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m--> 101\u001b[0m             \u001b[43mget_complex_request\u001b[49m(slot) \u001b[38;5;28;01mfor\u001b[39;00m slot \u001b[38;5;129;01min\u001b[39;00m slots  \u001b[38;5;66;03m# remove one of these if only wanting to download one thing at at time - usally a good idea to download multiple to speed up\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         ]\n\u001b[0;32m    103\u001b[0m     list_of_requests \u001b[38;5;241m=\u001b[39m [request\u001b[38;5;241m.\u001b[39mdownload_list[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m request \u001b[38;5;129;01min\u001b[39;00m list_of_requests]\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# Download data with multiple threads\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_complex_request' is not defined"
     ]
    }
   ],
   "source": [
    "#download part of the script\n",
    "# convert shape file data to int\n",
    "df_shape[\"POINT_ID\"]=df_shape[\"POINT_ID\"].astype(int)\n",
    "print(df_shape[\"POINT_ID\"])\n",
    "# it might be a good idea to choose only the ids that are associated with crops instead of downloading everything like i have\n",
    "# so match these two: \n",
    "df_crop=df_soil[df_soil[\"LC0_Desc\"]==\"Cropland\"]\n",
    "\n",
    "# idx=np.where(df_soil[\"LC0_Desc\"]==\"Cropland\")[0] # index of crops\n",
    "# crop_ids=np.array(df_soil.iloc[idx][\"POINTID\"]) # point ids which matches the df_shape\n",
    "\n",
    "\n",
    "# set i from 0 to how ever many shapes files there are (you might need to do multiple downloads sessions if your account runs out)\n",
    "for i in range(0,6326):\n",
    "    df_crop = df_crop.reset_index(drop=True)\n",
    "    # get soil id\n",
    "    crop_id=df_crop[\"POINTID\"][i]\n",
    "    samp_date=df_crop[\"SURVEY_DATE\"][i]\n",
    "    \n",
    "    # extract polygon and match id name\n",
    "    col_idx=np.where(df_shape[\"POINT_ID\"]==crop_id)[0]\n",
    "    id_name=df_shape.iloc[col_idx]['POINT_ID']\n",
    "    \n",
    "    # Check if col_idx is empty (length is 0)\n",
    "    if len(col_idx) == 0:\n",
    "    # Skip this iteration and move to the next\n",
    "        continue\n",
    "    \n",
    "    polygon_str = df_shape['geometry'][col_idx]\n",
    "    \n",
    "    coords_list = []\n",
    "    \n",
    "    for polygon in df_shape['geometry'][col_idx]:\n",
    "        # Extract the exterior coordinates of the polygon\n",
    "        exterior_coords = list(polygon.exterior.coords)\n",
    "    \n",
    "        # Append the exterior coordinates to the list\n",
    "        coords_list.append(exterior_coords)\n",
    "    \n",
    "    polygon = Polygon(coords_list[0])\n",
    "    \n",
    "    # Convert the Polygon into a MultiPolygon with a single polygon\n",
    "    multi_polygon = MultiPolygon([polygon])\n",
    "    \n",
    "    # Convert the MultiPolygon to GeoJSON format\n",
    "    geojson_data = json.dumps(mapping(multi_polygon))\n",
    "    \n",
    "    multi_polygon_geometry = json.loads(geojson_data) \n",
    "    \n",
    "    \n",
    "    dates=samp_month_fun(samp_date[3]+samp_date[4])\n",
    "    end=dates[1]\n",
    "    start=dates[0]\n",
    "    \n",
    "    end = end.split('-')\n",
    "    year = int(end[0])\n",
    "    month = int(end[1])\n",
    "    day = int(end[2])\n",
    "    \n",
    "    end=datetime.datetime(year, month, day)\n",
    "    \n",
    "    start = start.split('-')\n",
    "    year = int(start[0])\n",
    "    month = int(start[1])\n",
    "    day = int(start[2])\n",
    "    \n",
    "    start=datetime.datetime(year, month, day)\n",
    "    \n",
    "    n_chunks=30\n",
    "    tdelta = (end - start) / n_chunks\n",
    "    edges = [(start + i * tdelta).date().isoformat() for i in range(n_chunks)]\n",
    "    #slots = [(edges[i], edges[i + 1]) for i in range(len(edges) - 1)]\n",
    "    slots = [(edges[i], edges[i]) for i in range(len(edges) - 1)]\n",
    "    #print(slots)\n",
    "\n",
    "    # first evalscript to download from\n",
    "\n",
    "    def get_NDWI_request(time_interval):\n",
    "        return SentinelHubRequest(\n",
    "            evalscript=evalscript_all_nvdi, # set your evalscript to what you want\n",
    "            data_folder=\"sat_download\", # change to where you want your data output\n",
    "            input_data=[\n",
    "                SentinelHubRequest.input_data(\n",
    "                    data_collection=DataCollection.SENTINEL2_L2A,\n",
    "                    time_interval=time_interval,\n",
    "                    mosaicking_order=MosaickingOrder.LEAST_CC,\n",
    "                    other_args={'processing': {'upsampling': 'BILINEAR', 'downsampling':'BILINEAR'}},\n",
    "                    maxcc=0.2 # maxium cloud cover\n",
    "                )\n",
    "            ],\n",
    "            responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "            bbox=None,\n",
    "            geometry=Geometry(multi_polygon_geometry, CRS.WGS84),\n",
    "            size=(256, 256 ),  # Adjust the size of image as needed\n",
    "            config=config,\n",
    "        )\n",
    "    # second evalscript to download from (optional)\n",
    "\n",
    "    def get_complex_request(time_interval):\n",
    "        return SentinelHubRequest(\n",
    "            evalscript=evalscript_complex, # set your second evalscript to what you want\n",
    "            data_folder=\"/media/mabso/Data/downloads/soil_sen2_downloads\", # change to where you want your data output\n",
    "            input_data=[\n",
    "                SentinelHubRequest.input_data(\n",
    "                    data_collection=DataCollection.SENTINEL2_L2A,\n",
    "                    time_interval=time_interval,\n",
    "                    mosaicking_order=MosaickingOrder.LEAST_CC,\n",
    "                    other_args={'processing': {'upsampling': 'BILINEAR', 'downsampling':'BILINEAR'}},\n",
    "                    maxcc=0.25\n",
    "                )\n",
    "            ],\n",
    "            responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "            bbox=None,\n",
    "            geometry=Geometry(multi_polygon_geometry, CRS.WGS84),\n",
    "            size=(256, 256),  # Adjust the size as needed\n",
    "            config=config,\n",
    "        )\n",
    "    \n",
    "    # Create a list of requests\n",
    "    \n",
    "    list_of_requests = [\n",
    "            get_NDWI_request(slot) for slot in slots\n",
    "        ] + [\n",
    "            get_complex_request(slot) for slot in slots  # remove one of these if only wanting to download one thing at at time - usally a good idea to download multiple to speed up\n",
    "        ]\n",
    "    list_of_requests = [request.download_list[0] for request in list_of_requests]\n",
    "    \n",
    "    # Download data with multiple threads\n",
    "    data = SentinelHubDownloadClient(config=config).download(list_of_requests, max_threads=5)\n",
    "    \n",
    "    # note if you only choose to download one at a time you may need to do some major editing down below\n",
    "    \n",
    "    # remove all files with blanks\n",
    "    \n",
    "    def are_all_pixels_zero(image):\n",
    "        return np.all(image == 0)\n",
    "    \n",
    "    def process_folder(folder_path):\n",
    "        subfolders_to_delete = []\n",
    "        for root, subfolders, files in os.walk(folder_path):\n",
    "            images_have_only_zeros = True\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.tiff'):\n",
    "                    image_path = os.path.join(root, file)\n",
    "                    image = tiff.imread(image_path)\n",
    "                    if not are_all_pixels_zero(image):\n",
    "                        images_have_only_zeros = False\n",
    "                        break\n",
    "            \n",
    "            if images_have_only_zeros:\n",
    "                subfolders_to_delete.append(root)\n",
    "    \n",
    "        for subfolder in subfolders_to_delete:\n",
    "            print(f\"Deleting subfolder: {subfolder}\")\n",
    "            # Uncomment the line below to actually delete the subfolder and its contents\n",
    "            # shutil.rmtree(subfolder)\n",
    "            \n",
    "        return subfolders_to_delete\n",
    "    if __name__ == \"__main__\":\n",
    "        root_folder = \"sat_download\"\n",
    "        XX=process_folder(root_folder)\n",
    "        ''\n",
    "    for i in range(1,len(XX)):\n",
    "        shutil.rmtree(XX[i])\n",
    "        \n",
    "    # move files\n",
    "\n",
    "    \n",
    "    def process_folder(folder_path):\n",
    "        image_dict = {}  # To store matching image and mask paths\n",
    "    \n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.json'):\n",
    "                    json_path = os.path.join(root, file)\n",
    "                    with open(json_path) as json_file:\n",
    "                        json_data = json.load(json_file)\n",
    "    \n",
    "                    start_time = json_data[\"request\"][\"payload\"][\"input\"][\"data\"][0][\"dataFilter\"][\"timeRange\"][\"from\"]\n",
    "                    end_time = json_data[\"request\"][\"payload\"][\"input\"][\"data\"][0][\"dataFilter\"][\"timeRange\"][\"to\"]\n",
    "                    \n",
    "                    for tiff_file in files:\n",
    "                        if tiff_file.lower().endswith('.tiff') or tiff_file.lower().endswith('.tif'):\n",
    "                            tiff_path = os.path.join(root, tiff_file)\n",
    "                            image_dict.setdefault(start_time, []).append(tiff_path)\n",
    "    \n",
    "        for start_time, files in image_dict.items():\n",
    "            if len(files) == 2:\n",
    "                image_path = max(files, key=lambda x: os.path.getsize(x))\n",
    "                mask_path = min(files, key=lambda x: os.path.getsize(x))\n",
    "    \n",
    "                # Convert start_time to a more filesystem-friendly format\n",
    "                start_time_dt = datetime.strptime(start_time, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                formatted_start_time = start_time_dt.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "                \n",
    "                target_subfolder = os.path.join(folder_path, formatted_start_time)\n",
    "                os.makedirs(target_subfolder, exist_ok=True)\n",
    "                \n",
    "                # Rename and move the image and mask files\n",
    "                sr_image_path = os.path.join(target_subfolder, \"sr.tiff\") # name of first file\n",
    "                scl_mask_path = os.path.join(target_subfolder, \"scl.tiff\") # name of second files\n",
    "                shutil.move(image_path, sr_image_path)\n",
    "                shutil.move(mask_path, scl_mask_path)\n",
    "    \n",
    "                print(f\"Transferred to subfolder: {target_subfolder}\")\n",
    "                \n",
    "                # Delete the original subfolder\n",
    "                #   shutil.rmtree(os.path.dirname(image_path))\n",
    "                \n",
    "        print(\"Processing completed.\")\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        root_folder = \"sat_download\" # set to your root folder\n",
    "        process_folder(root_folder)\n",
    "    \n",
    "    # remove all non date time files\n",
    "    from datetime import datetime\n",
    "    \n",
    "    def is_datetime_format(string, format):\n",
    "        try:\n",
    "            datetime.strptime(string, format)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    def delete_non_datetime_folders(root_folder):\n",
    "        for folder in os.listdir(root_folder):\n",
    "            folder_path = os.path.join(root_folder, folder)\n",
    "            if os.path.isdir(folder_path) and not is_datetime_format(folder, \"%Y-%m-%d_%H-%M-%S\"):\n",
    "                print(f\"Deleting folder: {folder_path}\")\n",
    "                # Uncomment the line below to actually delete the folder and its contents\n",
    "                shutil.rmtree(folder_path)\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        root_folder = \"sat_download\" # set to your root folder\n",
    "        delete_non_datetime_folders(root_folder)\n",
    "    \n",
    "    \n",
    "    download_root_directory = \"sat_download\" # set to your root folder\n",
    "    \n",
    "    original_file_path_l=[]\n",
    "    new_file_path_l=[]\n",
    "    \n",
    "    inputs = glob.glob(download_root_directory+'/*/*', recursive=True)\n",
    "    \n",
    "    # Transpose the dimensions of each downloaded image file and write to new files\n",
    "    #for root, _, files in os.walk(download_root_directory):\n",
    "    for file_name in inputs:\n",
    "        if file_name.endswith(\"scl.tiff\"):  # Make sure to check the correct file extension\n",
    "            original_file_path = os.path.join(download_root_directory, file_name)\n",
    "            new_file_path = os.path.join(file_name, file_name.replace(\"scl.tiff\",\"scl1.tiff\"))\n",
    "            \n",
    "            # Open the raster file\n",
    "            with rasterio.open(original_file_path) as src:\n",
    "                data = src.read()\n",
    "                print(\"Original data shape:\", data.shape)\n",
    "                meta = src.meta.copy()  # Copy the metadata\n",
    "                \n",
    "            # Transpose dimensions from (4, h, w) to (h, w, 4)\n",
    "            data_transposed = data.transpose(1, 2, 0)\n",
    "            print(\"Transposed data shape:\", data_transposed.shape)\n",
    "            # Reorder bands to match the original order (B01, B02, B03, B04)\n",
    "            #data_reordered = data_transposed[:, :, [0, 1, 2, 3]]\n",
    "            \n",
    "            # Update the metadata\n",
    "            #  meta['count'] = 4  # Update the band count\n",
    "            \n",
    "            # Save the transposed data to the new file\n",
    "            with rasterio.open(new_file_path, 'w', **meta) as dst:\n",
    "                data_transposed = np.rollaxis(data_transposed, axis=2)\n",
    "                dst.write(data_transposed)  # Explicitly specify indexes\n",
    "            \n",
    "                \n",
    "            print(f\"Saved transposed data to {new_file_path}\")\n",
    "            \n",
    "            original_file_path_l.append(original_file_path)\n",
    "            \n",
    "            new_file_path_l.append(new_file_path)\n",
    "            # Rename the scl1.tiff to scl.tiff\n",
    "            #  os.rename(new_file_path, original_file_path)\n",
    "    \n",
    "            #  print(f\"Renamed {new_file_path} to {original_file_path}\")\n",
    "    \n",
    "    for i in range(0,len(original_file_path_l)):\n",
    "        os.remove(original_file_path_l[i])\n",
    "        os.rename(new_file_path_l[i], original_file_path_l[i])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    download_root_directory = \"sat_download\" # set to your root folder\n",
    "    inva_list=[]\n",
    "    \n",
    "    for root, dirs, files in os.walk(download_root_directory):\n",
    "        for folder in dirs:\n",
    "            try:\n",
    "                # Parse the original folder name\n",
    "                original_date = datetime.strptime(folder, \"%Y-%m-%d_%H-%M-%S\")\n",
    "                # Create the new folder name with the format %Y-%m-%d\n",
    "                new_folder_name = original_date.strftime(\"%Y-%m-%d\")\n",
    "                # Construct the paths\n",
    "                original_folder_path = os.path.join(root, folder)\n",
    "                new_folder_path = os.path.join(root, new_folder_name)\n",
    "                # Rename the folder\n",
    "                os.rename(original_folder_path, new_folder_path)\n",
    "                print(f\"Renamed folder: {original_folder_path} to {new_folder_path}\")\n",
    "            except ValueError:\n",
    "                print(f\"Skipped invalid folder name: {folder}\")\n",
    "                inva_list.append(i)\n",
    "    \n",
    "    \n",
    "    # read all folder names\n",
    "    root_dir=download_root_directory\n",
    "    inputs = glob.glob(root_dir+ '/*', recursive=True)\n",
    "    dates_array=np.sort(inputs)\n",
    "    \n",
    "    # Target date string\n",
    "    #samp_date_str = '06-07-18'\n",
    "    \n",
    "    # Convert the target date string to a datetime object\n",
    "    samp_date = datetime.strptime(samp_date, '%d-%m-%y')\n",
    "    \n",
    "    # Function to extract the date from a path string\n",
    "    def extract_date_from_path(path):\n",
    "        return datetime.strptime(path.split('/')[-1], '%Y-%m-%d')\n",
    "    \n",
    "    # Convert the array of dates to datetime objects\n",
    "    date_objects = np.array([extract_date_from_path(date_str) for date_str in dates_array])\n",
    "    \n",
    "    # Calculate the time differences between the target date and the array of dates\n",
    "    time_diffs = np.abs(date_objects - samp_date)\n",
    "    \n",
    "    # Find the index of the date with the smallest time difference\n",
    "    closest_date_index = np.argmin(time_diffs)\n",
    "    \n",
    "    # Get the closest date from the array\n",
    "    closest_date = dates_array[closest_date_index]\n",
    "    \n",
    "    print(\"Closest date:\", closest_date)\n",
    "    \n",
    "  \n",
    "    \n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', closest_date)\n",
    "    if match:\n",
    "        closest_date = match.group()\n",
    "    else:\n",
    "        closest_date = None\n",
    "    \n",
    "    \n",
    "    # List all files in the source directory\n",
    "    source_directory=download_root_directory\n",
    "    entries = os.listdir(source_directory)\n",
    "    \n",
    "    # Iterate through the entries and delete files and directories that don't contain the closest date\n",
    "    for entry in entries:\n",
    "        entry_path = os.path.join(source_directory, entry)\n",
    "        \n",
    "        if os.path.isdir(entry_path) and closest_date:\n",
    "            # Check if it's a directory and closest_date is defined\n",
    "            if closest_date not in entry:\n",
    "                # If the closest date is not in the directory name, remove it along with its contents\n",
    "                shutil.rmtree(entry_path)\n",
    "    \n",
    "    # List all files and directories in the source directory\n",
    "    entries = os.listdir(source_directory)\n",
    "    \n",
    "    \n",
    "    # Iterate through the entries and process directories\n",
    "    for entry in entries:\n",
    "        entry_path = os.path.join(source_directory, entry)\n",
    "        \n",
    "        if os.path.isdir(entry_path):\n",
    "            # Check if it's a directory\n",
    "            \n",
    "            if closest_date != samp_date:\n",
    "        # If the closest date is different from samp_date, include it in the directory name\n",
    "                new_dir_name = f\"{id_name.iloc[0]}_{closest_date}_close\"\n",
    "            else:\n",
    "        # Otherwise, use samp_date as is\n",
    "                new_dir_name = f\"{id_name.iloc[0]}_{samp_date}\"\n",
    "    \n",
    "            \n",
    "            # Rename the directory\n",
    "            new_dir_path = os.path.join(source_directory, new_dir_name)\n",
    "            os.rename(entry_path, new_dir_path)\n",
    "            \n",
    "            # Move the renamed directory to the specified folder\n",
    "            target_folder = \"ndvi_data\" # set to your folder name\n",
    "            target_path = os.path.join(target_folder, new_dir_name)\n",
    "            shutil.move(new_dir_path, target_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% read in what you have downloaded \n",
    "import math\n",
    "# find the ids that matches crops \n",
    "idx=np.where(df_soil[\"LC0_Desc\"]==\"Cropland\")[0]\n",
    "\n",
    "# get the point ids\n",
    "crop_ids=np.array(df_soil.iloc[idx][\"POINTID\"])\n",
    "\n",
    "#crop_ids=np.array(df_soil[\"POINTID\"])\n",
    "\n",
    "# match with NDVI images\n",
    "imgs=glob.glob(\"ndvi_data/*/sr.tiff\",recursive=True)\n",
    "#%% Extract ID numbers and dates of images\n",
    "id_numbers = []\n",
    "date_l=[]\n",
    "\n",
    "for img_path in imgs:\n",
    "    # Extract the folder name from the path\n",
    "    folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "    \n",
    "    # Split the folder name by underscore\n",
    "    parts = folder_name.split('_')\n",
    "    \n",
    "    # The ID number is the first part before the underscore\n",
    "    id_number = parts[0]\n",
    "    date=parts[1]\n",
    "    \n",
    "    # Append the ID number to the list\n",
    "    id_numbers.append(int(id_number))\n",
    "    date_l.append(date)\n",
    "\n",
    "id_numbers=np.array(id_numbers)\n",
    "\n",
    "#%% match idx and id numbers of the crops\n",
    "idx=np.where(np.isin(id_numbers,crop_ids))[0]\n",
    "date_crops_l=np.array(date_l)[idx] # date list\n",
    "id_crops_l=np.array(id_numbers)[idx] # id number list\n",
    "img_crops_l1=np.array(imgs)[idx] # list of images based on id number\n",
    "\n",
    "#%% plot your results to see what you have downloaded \n",
    "import tifffile as tiff\n",
    "\n",
    "test=tiff.imread(imgs[6])\n",
    "\n",
    "# plot what you have found\n",
    "plt.imshow(test)\n",
    "\n",
    "\n",
    "#%%\n",
    "import tifffile as tiff\n",
    "min_l=[]\n",
    "max_l=[]\n",
    "median_l=[]\n",
    "point_id_l=[]\n",
    "mean_l=[]\n",
    "date_l=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(img_crops_l1)):\n",
    "    test=tiff.imread(img_crops_l1[i])\n",
    " \n",
    "    test = np.where(test >= 0.0, test, np.nan) # this is only relevant for NDVI values\n",
    "    \n",
    "    min_value = np.nanmin(test)\n",
    "    max_value = np.nanmax(test)\n",
    "    median_value=np.nanmedian(test)\n",
    "    mean_value=np.nanmean(test)\n",
    "    std_value=np.nanstd(test)\n",
    "    pixel_count=np.sum(~np.isnan(test))\n",
    "    point_id=id_crops_l[i]\n",
    "    date=date_crops_l[i]\n",
    "    \n",
    "    min_l.append(min_value)\n",
    "    max_l.append(max_value) \n",
    "    median_l.append(median_value)\n",
    "    mean_l.append(mean_value)\n",
    "    point_id_l.append(point_id)\n",
    "    date_l.append(date)\n",
    "    \n",
    "data = {\n",
    "    'Min': min_l,\n",
    "    'Max': max_l,\n",
    "    'Median': median_l,\n",
    "    'Mean': mean_l,\n",
    "    'Point_id': point_id_l,\n",
    "    'date': date_l\n",
    "}    \n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#%% merge soil dataframe with NDVI frame\n",
    "idx_l=[]\n",
    "for i in range(0,len(df[\"Point_id\"])):\n",
    "    idx=np.where(df[\"Point_id\"][i]==np.array(df_soil[\"POINTID\"]))[0]\n",
    "    idx_l.append(int(idx))\n",
    "\n",
    "df_soil_subset=df_soil.iloc[idx_l]\n",
    "\n",
    "merged_df = pd.merge(df, df_soil_subset, left_on='Point_id', right_on='POINTID')\n",
    "\n",
    "#%% export CSV\n",
    "csv_file_path = \"lucas_soil_ndvi_new8_postive.csv\"\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "merged_df.to_csv(csv_file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
